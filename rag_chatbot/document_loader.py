import os
import re
import uuid

# Import PyMuPDF (the correct PDF library)
try:
    import pymupdf as fitz  # New PyMuPDF versions use this
except ImportError:
    try:
        import fitz  # Older PyMuPDF versions
        # Verify this is actually PyMuPDF by checking for Document class
        if not hasattr(fitz, 'Document'):
            raise ImportError("Wrong fitz package installed")
    except (ImportError, AttributeError):
        raise ImportError("PyMuPDF is required. Install with: pip install PyMuPDF")
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.document import Document

DOCS_PATH = 'company_docs'

def load_and_chunk_documents():
    """
    AkÄ±llÄ± MantÄ±ksal Gruplama YÃ¶ntemini Uygular:
    1. Her bir PDF'i tam bir metin olarak okur.
    2. Metni, "SECTION" ve "Case Studies" gibi ana mantÄ±ksal bloklara ayÄ±rÄ±r.
       Bu bloklar bizim "Parent Document"larÄ±mÄ±z (Ana DokÃ¼manlar) olur.
    3. Her bir tam ve anlamlÄ± Ana DokÃ¼manÄ±, aranabilir kÃ¼Ã§Ã¼k "Child" (Ã‡ocuk) parÃ§alara bÃ¶ler.
    """
    print("Starting Intelligent Logical Grouping process...")
    
    all_parent_documents = []
    
    # Her bir PDF dosyasÄ±nÄ± ayrÄ± ayrÄ± iÅŸleyeceÄŸiz
    for filename in os.listdir(DOCS_PATH):
        if not filename.lower().endswith('.pdf'):
            continue
            
        filepath = os.path.join(DOCS_PATH, filename)
        print(f"--> Processing document: {filename}")
        
        # Open PDF document
        doc = fitz.open(filepath)
        try:
            full_text = "".join(page.get_text() for page in doc)
        finally:
            doc.close()

        # --- ANA MANTIKSAL BLOKLARI (PARENT) OLUÅTURMA ---

        # 1. Ã–nce metni "SECTION" baÅŸlÄ±klarÄ±na gÃ¶re bÃ¼yÃ¼k parÃ§alara ayÄ±ralÄ±m.
        # Bu, dokÃ¼manÄ±n genel yapÄ±sÄ±nÄ± korur. `(?=SECTION \d)` ifadesi ayÄ±racÄ± silmeden bÃ¶ler.
        sections = re.split(r'(?=SECTION \d:)', full_text, flags=re.IGNORECASE)
        
        for section_text in sections:
            if not section_text.strip():
                continue
                
            # EÄŸer bÃ¶lÃ¼m "PROJECTS, SUCCESS STORIES" ise, onu daha da kÃ¼Ã§Ã¼k mantÄ±ksal
            # parÃ§alara, yani her bir vaka Ã§alÄ±ÅŸmasÄ±na ayÄ±ralÄ±m.
            if "PROJECTS, SUCCESS STORIES" in section_text[:100]:
                # Vaka Ã§alÄ±ÅŸmalarÄ± genellikle "â€¢ Ad (Kategori):" ile baÅŸlÄ±yor. Bu bizim anahtarÄ±mÄ±z.
                # GeliÅŸtirilmiÅŸ regex: hem tek kelime hem de boÅŸluklu markalarÄ± yakalar
                # Ã–rnek: "â€¢ LC WAIKIKI (", "â€¢ Migros (", "â€¢ Boyner (" vs.
                case_studies = re.split(r'(?=\nâ€¢\s*[A-ZÃ‡ÄÄ°Ä°Ã–ÅÃœ][A-ZÃ‡ÄÄ°Ä°Ã–ÅÃœ\s]*\()', section_text)
                
                # Ä°lk eleman genellikle bÃ¶lÃ¼m baÅŸlÄ±ÄŸÄ±dÄ±r, onu ayrÄ± bir parent yapalÄ±m
                if case_studies[0].strip():
                    all_parent_documents.append(Document(page_content=case_studies[0].strip(), metadata={"source": filename}))
                
                # Geri kalan her bir vaka Ã§alÄ±ÅŸmasÄ±nÄ± ayrÄ± bir parent yapalÄ±m
                for study_text in case_studies[1:]:
                    if study_text.strip():
                        all_parent_documents.append(Document(page_content=study_text.strip(), metadata={"source": filename}))
            else:
                # DiÄŸer bÃ¶lÃ¼mleri tek bir bÃ¼yÃ¼k parent olarak ekleyelim
                all_parent_documents.append(Document(page_content=section_text.strip(), metadata={"source": filename}))

    # --- ARANABÄ°LÄ°R Ã‡OCUK PARÃ‡ALARI (CHILD) OLUÅTURMA ---
    
    child_documents = []
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,  # Smaller chunks for better precision
        chunk_overlap=100,  # More overlap to preserve context
        separators=["\n\n", "\n", ". ", ", ", " "]
    )
    
    for parent_doc in all_parent_documents:
        parent_id = str(uuid.uuid4())
        parent_doc.metadata['doc_id'] = parent_id
        
        # Ana dokÃ¼manÄ±n tam metninden kÃ¼Ã§Ã¼k, aranabilir Ã§ocuk parÃ§alar oluÅŸturuyoruz
        smaller_chunks = child_splitter.split_text(parent_doc.page_content)
        
        for chunk_text in smaller_chunks:
            # Her Ã§ocuÄŸa, ana dokÃ¼manÄ±n referansÄ±nÄ± ve metadatasÄ±nÄ± kopyalÄ±yoruz
            child_metadata = parent_doc.metadata.copy()
            child_documents.append(Document(page_content=chunk_text, metadata=child_metadata))

    print(f"Total Parent Chunks (logical blocks) created: {len(all_parent_documents)}")
    print(f"Total Child Chunks for indexing created: {len(child_documents)}")
    print("Document loading and chunking finished successfully. ğŸš€")
    
    # Fonksiyonun Ã§Ä±ktÄ±sÄ± diÄŸer dosyalarla uyumlu: (parent_list, child_list)
    return all_parent_documents, child_documents